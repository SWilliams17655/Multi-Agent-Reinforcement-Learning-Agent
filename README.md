# Multi-Agent Reinforcement Learning (MARL) for Swarm Intelligence
  
Introduction: As computer scientists and engineers in the field of artificial intelligence, much of the inspiration for different algorithms necessary to solve complex problems has been drawn from nature. Through centuries of evolution and learning, nature has developed its own catalog of algorithms to address complex and dynamic problems, ensuring each species’ ability to survive in an ever-changing environment against a breadth of threats. Drawing from this inspiration, concepts like neural networks, expert systems, and evolutionary algorithms have revolutionized how we address many challenges. This article seeks to dive deeper into one of those concepts, swarm intelligence. With the development of small relatively cheap processors with onboard communications, the potential of swarm intelligence has grown over the last five years and warrants more research and development.

<p><b>Problem Statement</b>: One of the biggest challenges of developing a swarm intelligent algorithm is the steps necessary to engineer swarm behavior to achieve the desired emergent behavior against an almost infinite combination of environmental variables. This paper will seek to address that challenge by demonstrating how reinforcement learning can be used to train a swarm intelligent algorithm. First, it will provide an overview of the concept behind swarm intelligence and reinforcement learning. Next, it offers a demonstration of how a multi-agent reinforcement learning can assist in engineering a swarm intelligent solution to address a optimum path problem. Finally, comparing the performance of a human engineered swarm algorithm to one engineered through reinforcement learning we can see the advantages and disadvantages of this concept.</p>
<p><b>Why Swarm Intelligence?</b> Each algorithm across the artificial intelligence taxonomy is optimized for a particular problem; therefore, an answer we need to address up front is what value swarm intelligent algorithms offer. Swarm intelligent algorithms are particularly useful at solving search and optimization problems; optimizing a swarm’s behavior to find the global maxima or minima based on a pre-established criterion. In nature we see examples of swarm intelligent behavior enhancing the chance of survival for hundreds of animal species including bird flocks, fish schooling, ant colonies, and bee swarms . As an example, most have seen similar behavior to the flock of starling birds in the figure below.</p>

<div align='center'>
  <img src="https://github.com/SWilliams17655/Reinforcement-Learning-Agent/assets/114768010/b79d4760-b281-4485-a2d3-04c76a33a30b">
</div>

The complex synchronization of their flight is impressive, but what is more important to understand is how that emergent behavior came about. The flock is implementing a complex problem-solving process, optimizing the flock’s behavior to search for food and avoid threats. When one bird detects a threat, the entire flock reacts and avoids that threat. When they detect food or a place to land, the entire flock follows. The flock accomplishes all of this without a leader by using a local and decentralized network of communication where each bird is only communicating with his neighbor.3 Instead of a leader, the swarm’s complex behavior is the result of the established rules each bird follows.4 Observing these behavior, scientists have determined certain attributes exist across all swarms: first, agents communicate, second there is no leader, and third agents follow a pre-established set of rules. This is somewhat of an oversimplification of the bird’s behavior, but in this case, the birds are following a more complex variation of a particle swarm optimization (PSO) algorithm allowing the birds to search a large area then rapidly concentrate on a global maximum; food. [^6]

<div align='center'>
 <img src=https://github.com/SWilliams17655/Reinforcement-Learning-Agent/assets/114768010/ca169568-0974-4c80-9759-1cfc41e39121)>
</div>

<p>Engineering swarm intelligent behavior? As discussed earlier, there is no leader in a swarm, the swarm’s behavior emerges from a pre-established set of rules each agent follows. In his book Making Thinks Work, Solving Complex Problems in a Complex World, Yaneer Bar-Yam defines this relationship between the macro and the micro-scale as emergence or in this case emergent behavior.40 This concept of emergence is the true benefit and strength of any swarm intelligent algorithm because it allows the agent’s behavior at the micro-scale to shape and adapt the swarm’s behavior at the macro. To demonstrate this concept in this article, we  will create a problem the swarm must solve. In this case it is a logistics problem requiring the swarm find the optimum (fastest) path through terrain generated randomly via Perlin noise to resupply multiple points in the environment.  </p>

<div align='center'>
 <img src=https://github.com/SWilliams17655/Reinforcement-Learning-Agent/assets/114768010/73a6d531-62fa-436f-96cf-ecd9b2f84955)>
</div>

<p> Solution #1: Human Engineered Swarm Behavior: The behavior of the overall swarm is implemented using a set of rules at the agent or micro level that developed into macro level emergent behavior. To accomplish this, the vector for agents A1:n and actions of each agent in the swarm is calculated by mathematically combining a series of vectors for each agent: </p>
<p> Fitness Score: ƒ(An) The fitness of each agent is represented as the average value of surrounding terrain and can be between 0.0 and 1.0. </p>
<p> Cohesion Vector: Cv(An) Points towards the average center mass of agents with a ƒ( ) > 0.3 causing agents to remain cohesively consolidated into a singular mass. </p>
<p> Threat Vector: Tv(An)  Points away from average center of mass for agents with ƒ( ) < 0.3 causing agent to avoid traps.</p>
<p> Destination Vector: DV(x) This vector points in the direction of the target if it is in range or the closest agent with the highest fitness score FS(Agent or Threat). How the fitness score is calculated and propagated through the swarm will be discussed more later. </p>
<p> Random Vector RV(x): To smooth behavior and limit the risk of a swarm getting trapped, this simulation used a random vector in each agent which improved overall performance giving each agent a small element of free movement. </p> 
<p> Heading Vector: HV(x) This is the agent’s current heading which is calculated during each timing cycle. The algorithm used to calculate this vector using all other vectors is outlined later. </p>
<p> H_v (A_n )=C_v (A_n )*C_w+T_v (A_n )*T_w+D_v (A_n )*D_w+R_v (A_n )*R_(w ) </p>

[^6]: My reference.
